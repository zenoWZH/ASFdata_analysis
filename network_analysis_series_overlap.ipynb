{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import modin.pandas as mipd\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# technical nets are unweighted\n",
    "def get_tech_net(path):\n",
    "\n",
    "    bipartite_G = nx.Graph()\n",
    "    df = pd.read_csv(path, header=None, sep='##', engine='python')\n",
    "    df.columns = ['file', 'dev', 'weight']\n",
    "\n",
    "    ## Logic to add nodes and edges to graph with their metadata\n",
    "    for _, row in df.iterrows():\n",
    "        dev_node = row['dev']\n",
    "        file_node = row['file'].replace('   (with props)', '')\n",
    "        bipartite_G.add_node(dev_node, bipartite='dev')\n",
    "        bipartite_G.add_node(file_node, bipartite='file')\n",
    "        bipartite_G.add_edge(dev_node, file_node)\n",
    "\n",
    "    dev_nodes = {n for n, d in bipartite_G.nodes(data=True) if d[\"bipartite\"] == 'dev'}\n",
    "    file_nodes = {n for n, d in bipartite_G.nodes(data=True) if d[\"bipartite\"] == 'file'}\n",
    "    \n",
    "    return bipartite_G\n",
    "def cal_tech_net(path):\n",
    "    # check if file does not exist or empty\n",
    "    if not os.path.exists(path) or os.stat(path).st_size == 0:\n",
    "        return {'t_num_dev_nodes':0,\\\n",
    "                't_num_file_nodes':0,\\\n",
    "                't_num_dev_per_file':0,\\\n",
    "                't_num_file_per_dev':0,\\\n",
    "                't_graph_density':0,\\\n",
    "                't_dev_nodes': set()}\n",
    "\n",
    "    bipartite_G = get_tech_net(path)\n",
    "\n",
    "    graph_density = bipartite.density(bipartite_G, dev_nodes)\n",
    "    file_degrees, dev_degrees = bipartite.degrees(bipartite_G, dev_nodes)\n",
    "\n",
    "    num_file_nodes = len(file_degrees)\n",
    "    num_dev_nodes = len(dev_degrees)\n",
    "    file_node_degree = sum([degree for node, degree in file_degrees])/len(file_degrees)\n",
    "    dev_node_degree = sum([degree for node, degree in dev_degrees])/len(dev_degrees)\n",
    "\n",
    "    # return the features of tech net\n",
    "    return {'t_num_dev_nodes':num_dev_nodes,\\\n",
    "            't_num_file_nodes':num_file_nodes,\\\n",
    "            't_num_dev_per_file':file_node_degree,\\\n",
    "            't_num_file_per_dev':dev_node_degree,\\\n",
    "            't_graph_density':graph_density,\\\n",
    "            't_dev_nodes': set(dev_nodes)}\n",
    "\n",
    "def get_social_net(path):\n",
    "    G = nx.read_edgelist(path, create_using=nx.DiGraph(), nodetype=str, comments='*', delimiter='##', data=(('weight', int),))\n",
    "    return G\n",
    "\n",
    "# social nets are weighted\n",
    "def cal_social_net(path):\n",
    "    # if no network data\n",
    "    if not os.path.exists(path) or os.stat(path).st_size == 0:\n",
    "        return {'s_num_nodes':0, \\\n",
    "                's_dev_nodes':set(),\\\n",
    "                's_weighted_mean_degree':0,\\\n",
    "                's_num_component':0,\\\n",
    "                's_avg_clustering_coef':0,\\\n",
    "                's_largest_component':0,\\\n",
    "                's_graph_density':0}\n",
    "\n",
    "    # Processing features in social networks\n",
    "    G = nx.read_edgelist(path, create_using=nx.DiGraph(), nodetype=str, comments='*', delimiter='##', data=(('weight', int),))\n",
    "    # all dev nodes\n",
    "    dev_nodes = set(G.nodes)\n",
    "    # num. of total nodes\n",
    "    num_nodes = len(dev_nodes)\n",
    "    # weighted mean degree\n",
    "    degrees = G.degree(weight='weight')\n",
    "    weighted_mean_degree = sum([degree for node, degree in degrees])/num_nodes\n",
    "    # average clustering coefficient\n",
    "    avg_clustering_coef = nx.average_clustering(G)\n",
    "    # betweenness = nx.betweenness_centrality(G, weight='weight')\n",
    "    graph_density = nx.density(G)\n",
    "\n",
    "    G = nx.read_edgelist(path, create_using=nx.Graph(), nodetype=str, comments='*', delimiter='##', data=(('weight', int),))\n",
    "    # num. of dis-connected components\n",
    "    num_component = nx.number_connected_components(G)\n",
    "    # largest connected component\n",
    "    largest_component = len(max(nx.connected_components(G), key=len))\n",
    "    # num. of nodes in each component\n",
    "    # num_nodes_component = [list(c) for c in list(nx.connected_components(G))]\n",
    "\n",
    "    # return the features of the \n",
    "    return {'s_num_nodes': num_nodes,\\\n",
    "            's_dev_nodes': dev_nodes,\\\n",
    "            's_weighted_mean_degree':weighted_mean_degree,\\\n",
    "            's_num_component':num_component,\\\n",
    "            's_avg_clustering_coef':avg_clustering_coef,\\\n",
    "            's_largest_component':largest_component,\\\n",
    "            's_graph_density':graph_density}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incubating = pd.read_csv('/mnt/data0/lkyin/incubating.csv')\n",
    "df_graduated = pd.read_csv('/mnt/data0/lkyin/graduated.csv')\n",
    "df_retired = pd.read_csv('/mnt/data0/lkyin/retired.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description: Take a look at how many projects:\n",
    "    \n",
    "#### 1. Technical Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 4327 months\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4327/4327 [00:01<00:00, 3417.76it/s]\n"
     ]
    }
   ],
   "source": [
    "c_path = '/mnt/data0/lkyin/monthly_data/commits/'\n",
    "projects = os.listdir(c_path)\n",
    "print(\"Total:\", len(projects), \"months\")\n",
    "\n",
    "proj_names = []\n",
    "proj_ids= []\n",
    "proj_status = []\n",
    "for project in tqdm(projects):\n",
    "    project_name, period = project.replace('.csv', '').split('__')\n",
    "    proj_ids.append(project_name)\n",
    "    if project_name in df_incubating[\"alias\"].values :\n",
    "        proj_status.append(\"incubating\")\n",
    "        proj_names.append(str(df_incubating.loc[df_incubating[\"alias\"] == project_name][\"project_name\"].values[0]))\n",
    "    elif project_name in df_graduated[\"alias\"].values :\n",
    "        proj_status.append('graduated')\n",
    "        proj_names.append(str(df_graduated.loc[df_graduated[\"alias\"] == project_name][\"project_name\"].values[0]))\n",
    "    elif project_name in df_retired[\"alias\"].values :\n",
    "        proj_status.append('retired')\n",
    "        proj_names.append(str(df_retired.loc[df_retired[\"alias\"] == project_name][\"project_name\"].values[0]))\n",
    "    else:\n",
    "        proj_status.append('unknown')\n",
    "        proj_names.append('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count        4327\n",
      "unique        263\n",
      "top       JSPWiki\n",
      "freq           63\n",
      "dtype: object\n",
      "['retired' 'graduated']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_name</th>\n",
       "      <th>project_aliase</th>\n",
       "      <th>project_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Taverna</td>\n",
       "      <td>taverna</td>\n",
       "      <td>graduated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       project_name project_aliase project_status\n",
       "count           263            263            263\n",
       "unique          263            263              2\n",
       "top         Taverna        taverna      graduated\n",
       "freq              1              1            204"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech_proj_names = pd.Series(proj_names)\n",
    "print(tech_proj_names.describe())\n",
    "\n",
    "tech_proj_ids = pd.Series(proj_ids)\n",
    "\n",
    "tech_proj_status = pd.Series(proj_status)\n",
    "print(tech_proj_status.unique())\n",
    "\n",
    "df_proj_tech = pd.DataFrame()\n",
    "df_proj_tech['project_name'] = tech_proj_names\n",
    "df_proj_tech['project_aliase'] = tech_proj_ids\n",
    "df_proj_tech['project_status'] = tech_proj_status\n",
    "\n",
    "df_proj_tech = df_proj_tech.drop_duplicates()\n",
    "df_proj_tech.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Social Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 6036 months\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6036/6036 [00:01<00:00, 3512.87it/s]\n"
     ]
    }
   ],
   "source": [
    "e_path = '/mnt/data0/lkyin/monthly_data/emails/'\n",
    "projects = os.listdir(e_path)\n",
    "print(\"Total:\", len(projects), \"months\")\n",
    "\n",
    "proj_names = []\n",
    "proj_ids = []\n",
    "proj_status = []\n",
    "for project in tqdm(projects):\n",
    "    project_name, period = project.replace('.csv', '').split('__')\n",
    "    proj_ids.append(project_name)\n",
    "    if project_name in df_incubating[\"alias\"].values :\n",
    "        proj_status.append(\"incubating\")\n",
    "        proj_names.append(str(df_incubating.loc[df_incubating[\"alias\"] == project_name][\"project_name\"].values[0]))\n",
    "    elif project_name in df_graduated[\"alias\"].values :\n",
    "        proj_status.append('graduated')\n",
    "        proj_names.append(str(df_graduated.loc[df_graduated[\"alias\"] == project_name][\"project_name\"].values[0]))\n",
    "    elif project_name in df_retired[\"alias\"].values :\n",
    "        proj_status.append('retired')\n",
    "        proj_names.append(str(df_retired.loc[df_retired[\"alias\"] == project_name][\"project_name\"].values[0]))\n",
    "    else:\n",
    "        proj_status.append('unknown')\n",
    "        proj_names.append('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count            6036\n",
      "unique            272\n",
      "top       ODF Toolkit\n",
      "freq               87\n",
      "dtype: object\n",
      "['retired' 'graduated']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_name</th>\n",
       "      <th>project_aliase</th>\n",
       "      <th>project_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>272</td>\n",
       "      <td>272</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>272</td>\n",
       "      <td>272</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Taverna</td>\n",
       "      <td>taverna</td>\n",
       "      <td>graduated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       project_name project_aliase project_status\n",
       "count           272            272            272\n",
       "unique          272            272              2\n",
       "top         Taverna        taverna      graduated\n",
       "freq              1              1            211"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "social_proj_names = pd.Series(proj_names)\n",
    "print(social_proj_names.describe())\n",
    "\n",
    "social_proj_ids = pd.Series(proj_ids)\n",
    "\n",
    "social_proj_status = pd.Series(proj_status)\n",
    "print(social_proj_status.unique())\n",
    "\n",
    "df_proj_social = pd.DataFrame()\n",
    "df_proj_social['project_name'] = social_proj_names\n",
    "df_proj_social['project_aliase'] = social_proj_ids\n",
    "df_proj_social['project_status'] = social_proj_status\n",
    "\n",
    "df_proj_social = df_proj_social.drop_duplicates()\n",
    "df_proj_social.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_unite(net_tech, net_social, net_mix):\n",
    "\t# Merge tech network and social network by edgelist files\n",
    "\tif not os.path.exists(net_tech):\n",
    "\t\treturn None\n",
    "\tif not os.path.exists(net_social):\n",
    "\t\treturn None\n",
    "\n",
    "\tnet1_set = []\n",
    "\twith open(net_tech, 'r') as f:\n",
    "\t\tlines = f.read().splitlines()\n",
    "\t# print([net1, net2])\n",
    "\tfor line in lines:\n",
    "\t\tsender, recivier, weight = line.split('##')\n",
    "\t\tnet1_set.append([recivier, sender, weight])\n",
    "\tnet2_set = []\n",
    "\twith open(net_social, 'r') as f:\n",
    "\t\tlines = f.read().splitlines()\n",
    "\tfor line in lines:\n",
    "\t\tsender, recivier, weight = line.split('##')\t\t\n",
    "\t\tnet1_set.append([sender, recivier, weight])\n",
    "\n",
    "\t#print(net_mix_set)\n",
    "\twith open(net_mix, 'w') as f:\n",
    "\t\tfor sender, recivier, weight in net1_set:\n",
    "\t\t\tf.write(sender+\"##\"+recivier+\"##\"+weight+\"\\n\")\n",
    "\n",
    "\n",
    "\treturn net1_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using overlap coefficient\n",
    "def get_netseries_overlap(netnameseries, timeintervals):\n",
    "\n",
    "    netseries = []\n",
    "    for netname in netnameseries:\n",
    "        if not os.path.exists(netname):\n",
    "            #print(\"Not Exist File:\"+netname)\n",
    "            #netseries.append(set())\n",
    "            continue\n",
    "            # Don't Return NONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            #return None\n",
    "        else:\n",
    "            net_set = set()\n",
    "            with open(netname, 'r') as f:\n",
    "                lines = f.read().splitlines()\n",
    "            for line in lines:\n",
    "                sender, recivier, weight = line.split('##')\n",
    "                net_set.add((sender, recivier))\n",
    "            netseries.append(net_set)\n",
    "            \n",
    "    overlaplevelseries = []\n",
    "    interlevelseries = []\n",
    "    minlevelseries = []\n",
    "\n",
    "    nets = netseries\n",
    "    interlevelseries.append(nets)\n",
    "\n",
    "    minseries = []\n",
    "    for net_set in netseries:\n",
    "        minseries.append(len(net_set))\n",
    "    #minlevelseries.append(minseries)\n",
    "\n",
    "    for level in range(1, timeintervals):\n",
    "        \n",
    "        overlapseries = []\n",
    "        interseries = []\n",
    "        for i in range(len(nets)-1):\n",
    "\n",
    "            net1_set = nets[i]\n",
    "            net2_set = nets[i+1]\n",
    "\n",
    "            net_intersection = net1_set.intersection(net2_set)\n",
    "            interseries.append(net_intersection)\n",
    "            if min(minseries[i:i+level])!= 0:\n",
    "                overlapseries.append(len(net_intersection)/min(minseries[i:i+level]))\n",
    "            else:\n",
    "                overlapseries.append(0)\n",
    "        \n",
    "        overlaplevelseries.append(overlapseries)\n",
    "        # for next round of for\n",
    "        nets = interseries\n",
    "        interlevelseries.append(interseries)\n",
    "\n",
    "    return overlaplevelseries\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_resolution = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/mnt/data0/proj_osgeo/data_ASF_trial/network_data'+str(time_resolution)+'/'\n",
    "\n",
    "### Faster\n",
    "\n",
    "c_path = './network_data'+str(time_resolution)+'/commits/'\n",
    "e_path = './network_data'+str(time_resolution)+'/emails/'\n",
    "#c_path = './network_data/commits/'\n",
    "\n",
    "projects = os.listdir(c_path)\n",
    "project_names = [x.split('__')[0] for x in projects]\n",
    "project_names = pd.Series(project_names).drop_duplicates().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_path = data_path+\"mix/\"\n",
    "if not os.path.exists(mix_path):\n",
    "    os.makedirs(mix_path)\n",
    "\n",
    "for g_file in os.listdir(c_path):\n",
    "    net_mix_set = g_unite(c_path+g_file, e_path+g_file, mix_path+g_file)\n",
    "    #print(net_mix_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "projects = os.listdir(c_path)\n",
    "project_names = [x.split('__')[0] for x in projects]\n",
    "project_names = pd.Series(project_names).drop_duplicates().values\n",
    "\n",
    "df_tech_overlaps = pd.DataFrame()\n",
    "the_path = c_path\n",
    "for projid in project_names:\n",
    "    netlist = []\n",
    "    \n",
    "    for seq_num in range(time_resolution, 43, time_resolution):\n",
    "        #seq_num = 1\n",
    "        this_fname = the_path+projid+'__'+str(seq_num)+\".edgelist\"\n",
    "        netlist.append(this_fname)\n",
    "    if not os.path.exists(netlist[0]):\n",
    "        netlist.pop(0)\n",
    "    overlaps = get_netseries_overlap(netlist, timeintervals = 3)\n",
    "\n",
    "    projname = str(df_proj_tech.loc[df_proj_tech[\"project_aliase\"] == projid][\"project_name\"].values[0])\n",
    "    #print(projname)\n",
    "    df_tech_overlaps[projname] = pd.Series(overlaps) \n",
    "df_tech_overlaps.to_csv(\"./tech_overlaps_series\"+str(time_resolution)+\".csv\", index=None, sep=',')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "projects = os.listdir(e_path)\n",
    "project_names = [x.split('__')[0] for x in projects]\n",
    "project_names = pd.Series(project_names).drop_duplicates().values\n",
    "\n",
    "df_social_overlaps = pd.DataFrame()\n",
    "the_path = e_path\n",
    "for projid in project_names:\n",
    "    netlist = []\n",
    "    \n",
    "    for seq_num in range(time_resolution, 43, time_resolution):\n",
    "        #seq_num = 1\n",
    "        this_fname = the_path+projid+'__'+str(seq_num)+\".edgelist\"\n",
    "        netlist.append(this_fname)\n",
    "    if not os.path.exists(netlist[0]):\n",
    "        netlist.pop(0)\n",
    "    overlaps = get_netseries_overlap(netlist, timeintervals = 3)\n",
    "    \n",
    "    \n",
    "    projname = str(df_proj_social.loc[df_proj_social[\"project_aliase\"] == projid][\"project_name\"].values[0])\n",
    "    #print(projname)\n",
    "    df_social_overlaps[projname] = pd.Series(overlaps) \n",
    "df_social_overlaps.to_csv(\"./social_overlaps_series\"+str(time_resolution)+\".csv\", index=None, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "projects = os.listdir('./network_data'+str(time_resolution)+'/mix/')\n",
    "project_names = [x.split('__')[0] for x in projects]\n",
    "project_names = pd.Series(project_names).drop_duplicates().values\n",
    "\n",
    "df_mix_overlaps = pd.DataFrame()\n",
    "the_path = './network_data'+str(time_resolution)+'/mix/'\n",
    "for projid in project_names:\n",
    "    netlist = []\n",
    "    \n",
    "    for seq_num in range(time_resolution, 43, time_resolution):\n",
    "        #seq_num = 1\n",
    "        this_fname = the_path+projid+'__'+str(seq_num)+\".edgelist\"\n",
    "        netlist.append(this_fname)\n",
    "    if not os.path.exists(netlist[0]):\n",
    "        netlist.pop(0)\n",
    "    overlaps = get_netseries_overlap(netlist, timeintervals = 3)\n",
    "    \n",
    "    projname = str(df_proj_tech.loc[df_proj_tech[\"project_aliase\"] == projid][\"project_name\"].values[0])\n",
    "    #print(projname)\n",
    "    df_mix_overlaps[projname] = pd.Series(overlaps) \n",
    "df_mix_overlaps.to_csv(\"./mix_overlaps_series\"+str(time_resolution)+\".csv\", index=None, sep=',')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c125d54415bf97f751e5832101874a2f084e84c850663dda323e1a70b27f8b7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('3.8.10': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

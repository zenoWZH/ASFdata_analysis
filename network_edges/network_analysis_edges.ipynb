{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import modin.pandas as mipd\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# technical nets are unweighted\n",
    "def get_tech_net(path):\n",
    "\n",
    "    bipartite_G = nx.Graph()\n",
    "    df = pd.read_csv(path, header=None, sep='##', engine='python')\n",
    "    df.columns = ['file', 'dev', 'weight']\n",
    "\n",
    "    ## Logic to add nodes and edges to graph with their metadata\n",
    "    for _, row in df.iterrows():\n",
    "        dev_node = row['dev']\n",
    "        file_node = row['file'].replace('   (with props)', '')\n",
    "        bipartite_G.add_node(dev_node, bipartite='dev')\n",
    "        bipartite_G.add_node(file_node, bipartite='file')\n",
    "        bipartite_G.add_edge(dev_node, file_node)\n",
    "\n",
    "    dev_nodes = {n for n, d in bipartite_G.nodes(data=True) if d[\"bipartite\"] == 'dev'}\n",
    "    file_nodes = {n for n, d in bipartite_G.nodes(data=True) if d[\"bipartite\"] == 'file'}\n",
    "    \n",
    "    return bipartite_G\n",
    "def cal_tech_net(path):\n",
    "    # check if file does not exist or empty\n",
    "    if not os.path.exists(path) or os.stat(path).st_size == 0:\n",
    "        return {'t_num_dev_nodes':0,\\\n",
    "                't_num_file_nodes':0,\\\n",
    "                't_num_dev_per_file':0,\\\n",
    "                't_num_file_per_dev':0,\\\n",
    "                't_graph_density':0,\\\n",
    "                't_dev_nodes': set()}\n",
    "\n",
    "    bipartite_G = get_tech_net(path)\n",
    "\n",
    "    graph_density = bipartite.density(bipartite_G, dev_nodes)\n",
    "    file_degrees, dev_degrees = bipartite.degrees(bipartite_G, dev_nodes)\n",
    "\n",
    "    num_file_nodes = len(file_degrees)\n",
    "    num_dev_nodes = len(dev_degrees)\n",
    "    file_node_degree = sum([degree for node, degree in file_degrees])/len(file_degrees)\n",
    "    dev_node_degree = sum([degree for node, degree in dev_degrees])/len(dev_degrees)\n",
    "\n",
    "    # return the features of tech net\n",
    "    return {'t_num_dev_nodes':num_dev_nodes,\\\n",
    "            't_num_file_nodes':num_file_nodes,\\\n",
    "            't_num_dev_per_file':file_node_degree,\\\n",
    "            't_num_file_per_dev':dev_node_degree,\\\n",
    "            't_graph_density':graph_density,\\\n",
    "            't_dev_nodes': set(dev_nodes)}\n",
    "\n",
    "def get_social_net(path):\n",
    "    G = nx.read_edgelist(path, create_using=nx.DiGraph(), nodetype=str, comments='*', delimiter='##', data=(('weight', int),))\n",
    "    return G\n",
    "\n",
    "# social nets are weighted\n",
    "def cal_social_net(path):\n",
    "    # if no network data\n",
    "    if not os.path.exists(path) or os.stat(path).st_size == 0:\n",
    "        return {'s_num_nodes':0, \\\n",
    "                's_dev_nodes':set(),\\\n",
    "                's_weighted_mean_degree':0,\\\n",
    "                's_num_component':0,\\\n",
    "                's_avg_clustering_coef':0,\\\n",
    "                's_largest_component':0,\\\n",
    "                's_graph_density':0}\n",
    "\n",
    "    # Processing features in social networks\n",
    "    G = nx.read_edgelist(path, create_using=nx.DiGraph(), nodetype=str, comments='*', delimiter='##', data=(('weight', int),))\n",
    "    # all dev nodes\n",
    "    dev_nodes = set(G.nodes)\n",
    "    # num. of total nodes\n",
    "    num_nodes = len(dev_nodes)\n",
    "    # weighted mean degree\n",
    "    degrees = G.degree(weight='weight')\n",
    "    weighted_mean_degree = sum([degree for node, degree in degrees])/num_nodes\n",
    "    # average clustering coefficient\n",
    "    avg_clustering_coef = nx.average_clustering(G)\n",
    "    # betweenness = nx.betweenness_centrality(G, weight='weight')\n",
    "    graph_density = nx.density(G)\n",
    "\n",
    "    G = nx.read_edgelist(path, create_using=nx.Graph(), nodetype=str, comments='*', delimiter='##', data=(('weight', int),))\n",
    "    # num. of dis-connected components\n",
    "    num_component = nx.number_connected_components(G)\n",
    "    # largest connected component\n",
    "    largest_component = len(max(nx.connected_components(G), key=len))\n",
    "    # num. of nodes in each component\n",
    "    # num_nodes_component = [list(c) for c in list(nx.connected_components(G))]\n",
    "\n",
    "    # return the features of the \n",
    "    return {'s_num_nodes': num_nodes,\\\n",
    "            's_dev_nodes': dev_nodes,\\\n",
    "            's_weighted_mean_degree':weighted_mean_degree,\\\n",
    "            's_num_component':num_component,\\\n",
    "            's_avg_clustering_coef':avg_clustering_coef,\\\n",
    "            's_largest_component':largest_component,\\\n",
    "            's_graph_density':graph_density}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_unite(net_tech, net_social, net_mix):\n",
    "\t# Merge tech network and social network by edgelist files\n",
    "\tif not os.path.exists(net_tech):\n",
    "\t\treturn None\n",
    "\tif not os.path.exists(net_social):\n",
    "\t\treturn None\n",
    "\n",
    "\tnet1_set = []\n",
    "\twith open(net_tech, 'r') as f:\n",
    "\t\tlines = f.read().splitlines()\n",
    "\t# print([net1, net2])\n",
    "\tfor line in lines:\n",
    "\t\tsender, recivier, weight = line.split('##')\n",
    "\t\tnet1_set.append([recivier, sender, weight])\n",
    "\tnet2_set = []\n",
    "\twith open(net_social, 'r') as f:\n",
    "\t\tlines = f.read().splitlines()\n",
    "\tfor line in lines:\n",
    "\t\tsender, recivier, weight = line.split('##')\t\t\n",
    "\t\tnet1_set.append([sender, recivier, weight])\n",
    "\n",
    "\t#print(net_mix_set)\n",
    "\twith open(net_mix, 'w') as f:\n",
    "\t\tfor sender, recivier, weight in net1_set:\n",
    "\t\t\tf.write(sender+\"##\"+recivier+\"##\"+weight+\"\\n\")\n",
    "\n",
    "\n",
    "\treturn net1_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_continue_stats(netseries):\n",
    "    net_set = {}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_continue_stats(netseries):\n",
    "    max_set = {}\n",
    "    prev_set = {}\n",
    "    for netname in netseries:\n",
    "        count_set = {}\n",
    "        if not os.path.exists(netname):\n",
    "            #print(\"Not Exist File:\"+netname)\n",
    "            #netseries.append(set())\n",
    "            continue\n",
    "            # Don't Return NONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            #return None\n",
    "        else:\n",
    "            with open(netname, 'r') as f:\n",
    "                lines = f.read().splitlines()\n",
    "            for line in lines:\n",
    "                sender, receiver, weight = line.split('##')\n",
    "                if sender+\"##\"+receiver in prev_set:\n",
    "                    count_set[sender+\"##\"+receiver]=prev_set[sender+\"##\"+receiver]+1\n",
    "                else:\n",
    "                    count_set[sender+\"##\"+receiver]= 1\n",
    "        for edgekey in prev_set.keys():\n",
    "            if edgekey not in max_set:\n",
    "                max_set[edgekey]= prev_set[edgekey]\n",
    "            if edgekey not in count_set :\n",
    "                max_set[edgekey]= max(max_set[edgekey], prev_set[edgekey])\n",
    "        prev_set = count_set\n",
    "        #print(count_set)\n",
    "    for edgekey in prev_set.keys():\n",
    "        if edgekey not in max_set:\n",
    "            max_set[edgekey]= prev_set[edgekey]\n",
    "        else:\n",
    "            max_set[edgekey]= max(max_set[edgekey], prev_set[edgekey])\n",
    "    \n",
    "    edge_counts = list()\n",
    "    max_set_nums = list(max_set.values())\n",
    "    if len(max_set_nums)>0:\n",
    "        max_count = max(max_set_nums)\n",
    "        for i in range(max_count):\n",
    "            edge_counts.append(max_set_nums.count(i))\n",
    "                \n",
    "    gc.collect()\n",
    "    return edge_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_graduated = np.load('all_graduated.npy').tolist()\n",
    "all_retired = np.load('all_retired.npy').tolist()\n",
    "df_proj_tech = pd.read_csv(\"./df_proj_tech.csv\")\n",
    "df_proj_social = pd.read_csv(\"./df_proj_social.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_resolution = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './network_data'+str(time_resolution)+'/'\n",
    "\n",
    "### Faster\n",
    "\n",
    "c_path = './network_data'+str(time_resolution)+'/commits/'\n",
    "e_path = './network_data'+str(time_resolution)+'/emails/'\n",
    "#c_path = './network_data/commits/'\n",
    "\n",
    "projects = os.listdir(c_path)\n",
    "project_names = [x.split('__')[0] for x in projects]\n",
    "project_names = pd.Series(project_names).drop_duplicates().values\n",
    "\n",
    "mix_path = data_path+\"mix/\"\n",
    "if not os.path.exists(mix_path):\n",
    "    os.makedirs(mix_path)\n",
    "\n",
    "for g_file in os.listdir(c_path):\n",
    "    net_mix_set = g_unite(c_path+g_file, e_path+g_file, mix_path+g_file)\n",
    "    #print(net_mix_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 99/260 [00:04<00:06, 23.22it/s]PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      " 90%|█████████ | 234/260 [00:10<00:01, 25.54it/s]DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "100%|██████████| 260/260 [00:11<00:00, 23.14it/s]\n"
     ]
    }
   ],
   "source": [
    "projects = os.listdir(c_path)\n",
    "project_names = [x.split('__')[0] for x in projects]\n",
    "project_names = pd.Series(project_names).drop_duplicates().values\n",
    "\n",
    "\n",
    "df_tech_edges = pd.DataFrame()\n",
    "the_path = c_path\n",
    "for projid in tqdm(project_names):\n",
    "    netlist = []\n",
    "    for seq_num in range(0, 43, time_resolution):\n",
    "        #seq_num = 1\n",
    "        this_fname = the_path+projid+'__'+str(seq_num)+\".edgelist\"\n",
    "        netlist.append(this_fname)\n",
    "    proj_edge_stats = edge_continue_stats(netlist)\n",
    "    if not os.path.exists(netlist[0]):\n",
    "        netlist.pop(0)\n",
    "    projname = str(df_proj_tech.loc[df_proj_tech[\"project_aliase\"] == projid][\"project_name\"].values[0])\n",
    "    df_tech_edges[projname]= pd.Series(proj_edge_stats)\n",
    "\n",
    "df_tech_edges.to_csv(\"./tech_edges\"+str(time_resolution)+\".csv\", index=None, sep=',')        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 99/272 [00:04<00:06, 24.76it/s]PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "100%|██████████| 272/272 [00:12<00:00, 22.24it/s]\n"
     ]
    }
   ],
   "source": [
    "projects = os.listdir(e_path)\n",
    "project_names = [x.split('__')[0] for x in projects]\n",
    "project_names = pd.Series(project_names).drop_duplicates().values\n",
    "\n",
    "df_social_edges = pd.DataFrame()\n",
    "the_path = e_path\n",
    "for projid in tqdm(project_names):\n",
    "    netlist = []\n",
    "    for seq_num in range(0, 43, time_resolution):\n",
    "        #seq_num = 1\n",
    "        this_fname = the_path+projid+'__'+str(seq_num)+\".edgelist\"\n",
    "        netlist.append(this_fname)\n",
    "    proj_edge_stats = edge_continue_stats(netlist)\n",
    "    if not os.path.exists(netlist[0]):\n",
    "        netlist.pop(0)\n",
    "    try:\n",
    "        projname = str(df_proj_social.loc[df_proj_social[\"project_aliase\"] == projid][\"project_name\"].values[0])\n",
    "    except BaseException as err:\n",
    "        projname = projid\n",
    "    df_social_edges[projname]= pd.Series(proj_edge_stats)\n",
    "\n",
    "df_social_edges.to_csv(\"./social_edges\"+str(time_resolution)+\".csv\", index=None, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 99/259 [00:04<00:06, 23.94it/s]PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "100%|██████████| 259/259 [00:11<00:00, 23.03it/s]\n"
     ]
    }
   ],
   "source": [
    "projects = os.listdir('./network_data'+str(time_resolution)+'/mix/')\n",
    "project_names = [x.split('__')[0] for x in projects]\n",
    "project_names = pd.Series(project_names).drop_duplicates().values\n",
    "\n",
    "df_mix_edges = pd.DataFrame()\n",
    "the_path = './network_data'+str(time_resolution)+'/mix/'\n",
    "for projid in tqdm(project_names):\n",
    "    netlist = []\n",
    "    for seq_num in range(0, 43, time_resolution):\n",
    "        #seq_num = 1\n",
    "        this_fname = the_path+projid+'__'+str(seq_num)+\".edgelist\"\n",
    "        netlist.append(this_fname)\n",
    "    proj_edge_stats = edge_continue_stats(netlist)\n",
    "    if not os.path.exists(netlist[0]):\n",
    "        netlist.pop(0)\n",
    "    projname = str(df_proj_tech.loc[df_proj_tech[\"project_aliase\"] == projid][\"project_name\"].values[0])\n",
    "    df_mix_edges[projname]= pd.Series(proj_edge_stats)\n",
    "\n",
    "df_mix_edges.to_csv(\"./mix_edges\"+str(time_resolution)+\".csv\", index=None, sep=',')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8c5435d0e76923f90691df99ce64c3b05b292deff7dde5a7010d3193cf829434"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('3.8.11': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

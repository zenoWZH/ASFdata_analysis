{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import modin.pandas as mipd\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# technical nets are unweighted\n",
    "def get_tech_net(path):\n",
    "\n",
    "    bipartite_G = nx.Graph()\n",
    "    df = pd.read_csv(path, header=None, sep='##', engine='python')\n",
    "    df.columns = ['file', 'dev', 'weight']\n",
    "\n",
    "    ## Logic to add nodes and edges to graph with their metadata\n",
    "    for _, row in df.iterrows():\n",
    "        dev_node = row['dev']\n",
    "        file_node = row['file'].replace('   (with props)', '')\n",
    "        bipartite_G.add_node(dev_node, bipartite='dev')\n",
    "        bipartite_G.add_node(file_node, bipartite='file')\n",
    "        bipartite_G.add_edge(dev_node, file_node)\n",
    "\n",
    "    dev_nodes = {n for n, d in bipartite_G.nodes(data=True) if d[\"bipartite\"] == 'dev'}\n",
    "    file_nodes = {n for n, d in bipartite_G.nodes(data=True) if d[\"bipartite\"] == 'file'}\n",
    "    \n",
    "    return bipartite_G\n",
    "def cal_tech_net(path):\n",
    "    # check if file does not exist or empty\n",
    "    if not os.path.exists(path) or os.stat(path).st_size == 0:\n",
    "        return {'t_num_dev_nodes':0,\\\n",
    "                't_num_file_nodes':0,\\\n",
    "                't_num_dev_per_file':0,\\\n",
    "                't_num_file_per_dev':0,\\\n",
    "                't_graph_density':0,\\\n",
    "                't_dev_nodes': set()}\n",
    "\n",
    "    bipartite_G = get_tech_net(path)\n",
    "\n",
    "    graph_density = bipartite.density(bipartite_G, dev_nodes)\n",
    "    file_degrees, dev_degrees = bipartite.degrees(bipartite_G, dev_nodes)\n",
    "\n",
    "    num_file_nodes = len(file_degrees)\n",
    "    num_dev_nodes = len(dev_degrees)\n",
    "    file_node_degree = sum([degree for node, degree in file_degrees])/len(file_degrees)\n",
    "    dev_node_degree = sum([degree for node, degree in dev_degrees])/len(dev_degrees)\n",
    "\n",
    "    # return the features of tech net\n",
    "    return {'t_num_dev_nodes':num_dev_nodes,\\\n",
    "            't_num_file_nodes':num_file_nodes,\\\n",
    "            't_num_dev_per_file':file_node_degree,\\\n",
    "            't_num_file_per_dev':dev_node_degree,\\\n",
    "            't_graph_density':graph_density,\\\n",
    "            't_dev_nodes': set(dev_nodes)}\n",
    "\n",
    "def get_social_net(path):\n",
    "    G = nx.read_edgelist(path, create_using=nx.DiGraph(), nodetype=str, comments='*', delimiter='##', data=(('weight', int),))\n",
    "    return G\n",
    "\n",
    "# social nets are weighted\n",
    "def cal_social_net(path):\n",
    "    # if no network data\n",
    "    if not os.path.exists(path) or os.stat(path).st_size == 0:\n",
    "        return {'s_num_nodes':0, \\\n",
    "                's_dev_nodes':set(),\\\n",
    "                's_weighted_mean_degree':0,\\\n",
    "                's_num_component':0,\\\n",
    "                's_avg_clustering_coef':0,\\\n",
    "                's_largest_component':0,\\\n",
    "                's_graph_density':0}\n",
    "\n",
    "    # Processing features in social networks\n",
    "    G = nx.read_edgelist(path, create_using=nx.DiGraph(), nodetype=str, comments='*', delimiter='##', data=(('weight', int),))\n",
    "    # all dev nodes\n",
    "    dev_nodes = set(G.nodes)\n",
    "    # num. of total nodes\n",
    "    num_nodes = len(dev_nodes)\n",
    "    # weighted mean degree\n",
    "    degrees = G.degree(weight='weight')\n",
    "    weighted_mean_degree = sum([degree for node, degree in degrees])/num_nodes\n",
    "    # average clustering coefficient\n",
    "    avg_clustering_coef = nx.average_clustering(G)\n",
    "    # betweenness = nx.betweenness_centrality(G, weight='weight')\n",
    "    graph_density = nx.density(G)\n",
    "\n",
    "    G = nx.read_edgelist(path, create_using=nx.Graph(), nodetype=str, comments='*', delimiter='##', data=(('weight', int),))\n",
    "    # num. of dis-connected components\n",
    "    num_component = nx.number_connected_components(G)\n",
    "    # largest connected component\n",
    "    largest_component = len(max(nx.connected_components(G), key=len))\n",
    "    # num. of nodes in each component\n",
    "    # num_nodes_component = [list(c) for c in list(nx.connected_components(G))]\n",
    "\n",
    "    # return the features of the \n",
    "    return {'s_num_nodes': num_nodes,\\\n",
    "            's_dev_nodes': dev_nodes,\\\n",
    "            's_weighted_mean_degree':weighted_mean_degree,\\\n",
    "            's_num_component':num_component,\\\n",
    "            's_avg_clustering_coef':avg_clustering_coef,\\\n",
    "            's_largest_component':largest_component,\\\n",
    "            's_graph_density':graph_density}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incubating = pd.read_csv('/mnt/data0/lkyin/incubating.csv')\n",
    "df_graduated = pd.read_csv('/mnt/data0/lkyin/graduated.csv')\n",
    "df_retired = pd.read_csv('/mnt/data0/lkyin/retired.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Faster\n",
    "\n",
    "#c_path = './network_data'+str(time_resolution)+'/commits/'\n",
    "#c_path = './network_data/commits/'\n",
    "\n",
    "#projects = os.listdir(c_path)\n",
    "#project_names = [x.split('__')[0] for x in projects]\n",
    "#project_names = pd.Series(project_names).drop_duplicates().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description: Take a look at how many projects:\n",
    "    \n",
    "#### 1. Technical Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 4327 months\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4327/4327 [00:01<00:00, 3385.26it/s]\n"
     ]
    }
   ],
   "source": [
    "c_path = '/mnt/data0/lkyin/monthly_data/commits/'\n",
    "projects = os.listdir(c_path)\n",
    "print(\"Total:\", len(projects), \"months\")\n",
    "\n",
    "proj_names = []\n",
    "proj_ids= []\n",
    "proj_status = []\n",
    "for project in tqdm(projects):\n",
    "    project_name, period = project.replace('.csv', '').split('__')\n",
    "    proj_ids.append(project_name)\n",
    "    if project_name in df_incubating[\"alias\"].values :\n",
    "        proj_status.append(\"incubating\")\n",
    "        proj_names.append(str(df_incubating.loc[df_incubating[\"alias\"] == project_name][\"project_name\"].values[0]))\n",
    "    elif project_name in df_graduated[\"alias\"].values :\n",
    "        proj_status.append('graduated')\n",
    "        proj_names.append(str(df_graduated.loc[df_graduated[\"alias\"] == project_name][\"project_name\"].values[0]))\n",
    "    elif project_name in df_retired[\"alias\"].values :\n",
    "        proj_status.append('retired')\n",
    "        proj_names.append(str(df_retired.loc[df_retired[\"alias\"] == project_name][\"project_name\"].values[0]))\n",
    "    else:\n",
    "        proj_status.append('unknown')\n",
    "        proj_names.append('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count        4327\n",
      "unique        263\n",
      "top       JSPWiki\n",
      "freq           63\n",
      "dtype: object\n",
      "['retired' 'graduated']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_name</th>\n",
       "      <th>project_aliase</th>\n",
       "      <th>project_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Taverna</td>\n",
       "      <td>taverna</td>\n",
       "      <td>graduated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       project_name project_aliase project_status\n",
       "count           263            263            263\n",
       "unique          263            263              2\n",
       "top         Taverna        taverna      graduated\n",
       "freq              1              1            204"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech_proj_names = pd.Series(proj_names)\n",
    "print(tech_proj_names.describe())\n",
    "\n",
    "tech_proj_ids = pd.Series(proj_ids)\n",
    "\n",
    "tech_proj_status = pd.Series(proj_status)\n",
    "print(tech_proj_status.unique())\n",
    "\n",
    "df_proj_tech = pd.DataFrame()\n",
    "df_proj_tech['project_name'] = tech_proj_names\n",
    "df_proj_tech['project_aliase'] = tech_proj_ids\n",
    "df_proj_tech['project_status'] = tech_proj_status\n",
    "\n",
    "df_proj_tech = df_proj_tech.drop_duplicates()\n",
    "df_proj_tech.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Social Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 6036 months\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6036/6036 [00:01<00:00, 3849.82it/s]\n"
     ]
    }
   ],
   "source": [
    "e_path = '/mnt/data0/lkyin/monthly_data/emails/'\n",
    "projects = os.listdir(e_path)\n",
    "print(\"Total:\", len(projects), \"months\")\n",
    "\n",
    "proj_names = []\n",
    "proj_ids = []\n",
    "proj_status = []\n",
    "for project in tqdm(projects):\n",
    "    project_name, period = project.replace('.csv', '').split('__')\n",
    "    proj_ids.append(project_name)\n",
    "    if project_name in df_incubating[\"alias\"].values :\n",
    "        proj_status.append(\"incubating\")\n",
    "        proj_names.append(str(df_incubating.loc[df_incubating[\"alias\"] == project_name][\"project_name\"].values[0]))\n",
    "    elif project_name in df_graduated[\"alias\"].values :\n",
    "        proj_status.append('graduated')\n",
    "        proj_names.append(str(df_graduated.loc[df_graduated[\"alias\"] == project_name][\"project_name\"].values[0]))\n",
    "    elif project_name in df_retired[\"alias\"].values :\n",
    "        proj_status.append('retired')\n",
    "        proj_names.append(str(df_retired.loc[df_retired[\"alias\"] == project_name][\"project_name\"].values[0]))\n",
    "    else:\n",
    "        proj_status.append('unknown')\n",
    "        proj_names.append('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count            6036\n",
      "unique            272\n",
      "top       ODF Toolkit\n",
      "freq               87\n",
      "dtype: object\n",
      "['retired' 'graduated']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_name</th>\n",
       "      <th>project_aliase</th>\n",
       "      <th>project_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>272</td>\n",
       "      <td>272</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>272</td>\n",
       "      <td>272</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Taverna</td>\n",
       "      <td>taverna</td>\n",
       "      <td>graduated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       project_name project_aliase project_status\n",
       "count           272            272            272\n",
       "unique          272            272              2\n",
       "top         Taverna        taverna      graduated\n",
       "freq              1              1            211"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "social_proj_names = pd.Series(proj_names)\n",
    "print(social_proj_names.describe())\n",
    "\n",
    "social_proj_ids = pd.Series(proj_ids)\n",
    "\n",
    "social_proj_status = pd.Series(proj_status)\n",
    "print(social_proj_status.unique())\n",
    "\n",
    "df_proj_social = pd.DataFrame()\n",
    "df_proj_social['project_name'] = social_proj_names\n",
    "df_proj_social['project_aliase'] = social_proj_ids\n",
    "df_proj_social['project_status'] = social_proj_status\n",
    "\n",
    "df_proj_social = df_proj_social.drop_duplicates()\n",
    "df_proj_social.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlap Calculation now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sets_jaccard_unweighted(net1_set, net2_set):\n",
    "    intersection_edges = net1_set.intersection(net2_set)\n",
    "    return len(intersection_edges) / (len(net1_set) + len(net2_set)-len(intersection_edges))\n",
    "\n",
    "def sets_overlap_unweighted(net1_set, net2_set):\n",
    "    intersection_edges = net1_set.intersection(net2_set)\n",
    "    return len(intersection_edges) / min([len(net1_set), len(net2_set)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlap calculation from Likang\n",
    "# Jaccard Now\n",
    "def get_net_overlap(net1, net2):\n",
    "\t\n",
    "\tif not os.path.exists(net1):\n",
    "\t\treturn 0\n",
    "\tif not os.path.exists(net2):\n",
    "\t\treturn 0\n",
    "\n",
    "\tnet1_set = set()\n",
    "\twith open(net1, 'r') as f:\n",
    "\t\tlines = f.read().splitlines()\n",
    "\t# print([net1, net2])\n",
    "\tfor line in lines:\n",
    "\t\tsender, recivier, weight = line.split('##')\n",
    "\t\tnet1_set.add((sender, recivier))\n",
    "\tnet2_set = set()\n",
    "\twith open(net2, 'r') as f:\n",
    "\t\tlines = f.read().splitlines()\n",
    "\tfor line in lines:\n",
    "\t\tsender, recivier, weight = line.split('##')\t\t\n",
    "\t\tnet2_set.add((sender, recivier))\n",
    "\tif len(net1_set) == 0 or len(net2_set) == 0:\n",
    "\t\treturn 0\n",
    "\n",
    "\treturn sets_overlap_unweighted(net1_set, net2_set)\n",
    "\t#return sets_jaccard_unweighted(net1_set, net2_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlap calculation from Likang\n",
    "# Jaccard Now\n",
    "def get_net_jaccard(net1, net2):\n",
    "\t\n",
    "\tif not os.path.exists(net1):\n",
    "\t\treturn 0\n",
    "\tif not os.path.exists(net2):\n",
    "\t\treturn 0\n",
    "\n",
    "\tnet1_set = set()\n",
    "\twith open(net1, 'r') as f:\n",
    "\t\tlines = f.read().splitlines()\n",
    "\t# print([net1, net2])\n",
    "\tfor line in lines:\n",
    "\t\tsender, recivier, weight = line.split('##')\n",
    "\t\tnet1_set.add((sender, recivier))\n",
    "\tnet2_set = set()\n",
    "\twith open(net2, 'r') as f:\n",
    "\t\tlines = f.read().splitlines()\n",
    "\tfor line in lines:\n",
    "\t\tsender, recivier, weight = line.split('##')\t\t\n",
    "\t\tnet2_set.add((sender, recivier))\n",
    "\tif len(net1_set) == 0 or len(net2_set) == 0:\n",
    "\t\treturn 0\n",
    "\n",
    "\t#return sets_overlap_unweighted(net1_set, net2_set)\n",
    "\treturn sets_jaccard_unweighted(net1_set, net2_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_netseries_overlap(netnameseries, timeintervals):\n",
    "\n",
    "    netseries = []\n",
    "    for netname in netnameseries:\n",
    "        if not os.path.exists(netname):\n",
    "            print(\"Not Exist File:\"+netname)\n",
    "            netseries.append({})\n",
    "        else:\n",
    "            net_set = set()\n",
    "            with open(netname, 'r') as f:\n",
    "                lines = f.read().splitlines()\n",
    "            for line in lines:\n",
    "                sender, recivier, weight = line.split('##')\n",
    "                net_set.add((sender, recivier))\n",
    "            netseries.append(net_set)\n",
    "            \n",
    "    overlaplevelseries = []\n",
    "    interlevelseries = []\n",
    "    minlevelseries = []\n",
    "\n",
    "    nets = netseries\n",
    "    interlevelseries.append(nets)\n",
    "\n",
    "    minseries = []\n",
    "    for net_set in netseries:\n",
    "        minseries.append(len(net_set))\n",
    "    #minlevelseries.append(minseries)\n",
    "\n",
    "    for level in range(1, timeintervals):\n",
    "        \n",
    "        for i in range(len(nets)-level):\n",
    "            minseries[i] = min(minseries[i],minseries[i+1])\n",
    "        #minseries = []\n",
    "        #for i in range(len(nets)-level):\n",
    "        #    minseries.append(min(minlevelseries[level-1][i:i+1]))\n",
    "        #minlevelseries.append(minseries)\n",
    "        \n",
    "        overlapseries = []\n",
    "        interseries = []\n",
    "        for i in range(len(nets)-level):\n",
    "\n",
    "            net1_set = nets[i]\n",
    "            net2_set = nets[i+1]\n",
    "\n",
    "            net_intersection = net1_set.intersection(net2_set)\n",
    "            interseries.append(net_intersection)\n",
    "            overlapseries.append(len(net_intersection)/minseries[i])\n",
    "\n",
    "        overlaplevelseries.append(overlapseries)\n",
    "        # for next round of for\n",
    "        nets = interseries\n",
    "        interlevelseries.append(interseries)\n",
    "\n",
    "    return overlaplevelseries\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_unite(net_tech, net_social, net_mix):\n",
    "\t# Merge tech network and social network by edgelist files\n",
    "\tif not os.path.exists(net_tech):\n",
    "\t\treturn None\n",
    "\tif not os.path.exists(net_social):\n",
    "\t\treturn None\n",
    "\n",
    "\tnet1_set = []\n",
    "\twith open(net_tech, 'r') as f:\n",
    "\t\tlines = f.read().splitlines()\n",
    "\t# print([net1, net2])\n",
    "\tfor line in lines:\n",
    "\t\tsender, recivier, weight = line.split('##')\n",
    "\t\tnet1_set.append([recivier, sender, weight])\n",
    "\tnet2_set = []\n",
    "\twith open(net_social, 'r') as f:\n",
    "\t\tlines = f.read().splitlines()\n",
    "\tfor line in lines:\n",
    "\t\tsender, recivier, weight = line.split('##')\t\t\n",
    "\t\tnet1_set.append([sender, recivier, weight])\n",
    "\n",
    "\t#print(net_mix_set)\n",
    "\twith open(net_mix, 'w') as f:\n",
    "\t\tfor sender, recivier, weight in net1_set:\n",
    "\t\t\tf.write(sender+\"##\"+recivier+\"##\"+weight+\"\\n\")\n",
    "\n",
    "\n",
    "\treturn net1_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_resolution = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/mnt/data0/proj_osgeo/data_ASF_trial/network_data'+str(time_resolution)+'/'\n",
    "\n",
    "### Faster\n",
    "\n",
    "c_path = './network_data'+str(time_resolution)+'/commits/'\n",
    "e_path = './network_data'+str(time_resolution)+'/emails/'\n",
    "#c_path = './network_data/commits/'\n",
    "\n",
    "projects = os.listdir(c_path)\n",
    "project_names = [x.split('__')[0] for x in projects]\n",
    "project_names = pd.Series(project_names).drop_duplicates().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_path = data_path+\"mix/\"\n",
    "if not os.path.exists(mix_path):\n",
    "    os.makedirs(mix_path)\n",
    "\n",
    "for g_file in os.listdir(c_path):\n",
    "    net_mix_set = g_unite(c_path+g_file, e_path+g_file, mix_path+g_file)\n",
    "    #print(net_mix_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "projects = os.listdir(c_path)\n",
    "project_names = [x.split('__')[0] for x in projects]\n",
    "project_names = pd.Series(project_names).drop_duplicates().values\n",
    "\n",
    "df_tech_overlaps = pd.DataFrame()\n",
    "the_path = c_path\n",
    "for projid in project_names:\n",
    "    overlaps = []\n",
    "    prev_fname = the_path+projid+'__'+str(0)+\".edgelist\"\n",
    "    for seq_num in range(time_resolution, 63, time_resolution):\n",
    "        #seq_num = 1\n",
    "        this_fname = the_path+projid+'__'+str(seq_num)+\".edgelist\"\n",
    "        \n",
    "        #while os.path.exists(this_fname):\n",
    "        if os.path.exists(prev_fname):\n",
    "            overlaps.append(get_net_overlap(prev_fname, this_fname))\n",
    "        #seq_num+=1\n",
    "        prev_fname = this_fname\n",
    "        this_fname = the_path+projid+'__'+str(seq_num)+\".edgelist\"\n",
    "        \n",
    "    overlaps.append(-1.0)\n",
    "    for i in range(len(overlaps),65):\n",
    "        overlaps.append(-1.0)\n",
    "    projname = str(df_proj_tech.loc[df_proj_tech[\"project_aliase\"] == projid][\"project_name\"].values[0])\n",
    "    #print(projname)\n",
    "    df_tech_overlaps[projname] = pd.Series(overlaps) \n",
    "df_tech_overlaps.to_csv(\"./tech_overlaps\"+str(time_resolution)+\".csv\", index=None, sep=',')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "projects = os.listdir(c_path)\n",
    "project_names = [x.split('__')[0] for x in projects]\n",
    "project_names = pd.Series(project_names).drop_duplicates().values\n",
    "\n",
    "df_tech_overlaps = pd.DataFrame()\n",
    "the_path = c_path\n",
    "for projid in project_names:\n",
    "    overlaps = []\n",
    "    prev_fname = the_path+projid+'__'+str(0)+\".edgelist\"\n",
    "    for seq_num in range(time_resolution, 63, time_resolution):\n",
    "        #seq_num = 1\n",
    "        this_fname = the_path+projid+'__'+str(seq_num)+\".edgelist\"\n",
    "        \n",
    "        #while os.path.exists(this_fname):\n",
    "        if os.path.exists(prev_fname):\n",
    "            overlaps.append(get_net_jaccard(prev_fname, this_fname))\n",
    "        #seq_num+=1\n",
    "        prev_fname = this_fname\n",
    "        this_fname = the_path+projid+'__'+str(seq_num)+\".edgelist\"\n",
    "        \n",
    "    overlaps.append(-1.0)\n",
    "    for i in range(len(overlaps),65):\n",
    "        overlaps.append(-1.0)\n",
    "    projname = str(df_proj_tech.loc[df_proj_tech[\"project_aliase\"] == projid][\"project_name\"].values[0])\n",
    "    #print(projname)\n",
    "    df_tech_overlaps[projname] = pd.Series(overlaps) \n",
    "df_tech_overlaps.to_csv(\"./tech_jaccards\"+str(time_resolution)+\".csv\", index=None, sep=',')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Kitty', 'Tika', 'Lucene.NET', 'Pivot', 'Slider', 'Streams', 'Gossip',\n",
       "       'Unomi', 'ODF Toolkit', 'Tephra',\n",
       "       ...\n",
       "       'Wicket', 'Geode', 'WebWork 2', 'Ignite', 'Crunch', 'CommonsRDF',\n",
       "       'Directory', 'Kabuki', 'Concerted', 'Traffic Server'],\n",
       "      dtype='object', length=263)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tech_overlaps.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "projects = os.listdir(e_path)\n",
    "project_names = [x.split('__')[0] for x in projects]\n",
    "project_names = pd.Series(project_names).drop_duplicates().values\n",
    "\n",
    "df_social_overlaps = pd.DataFrame()\n",
    "the_path = e_path\n",
    "for projid in project_names:\n",
    "    overlaps = []\n",
    "    prev_fname = the_path+projid+'__'+str(0)+\".edgelist\"\n",
    "    for seq_num in range(time_resolution,87, time_resolution):\n",
    "        #seq_num = 1\n",
    "        this_fname = the_path+projid+'__'+str(seq_num)+\".edgelist\"\n",
    "        \n",
    "        #while os.path.exists(this_fname):\n",
    "        if os.path.exists(prev_fname):\n",
    "            overlaps.append(get_net_overlap(prev_fname, this_fname))\n",
    "        #seq_num+=1\n",
    "        prev_fname = this_fname\n",
    "        this_fname = the_path+projid+'__'+str(seq_num)+\".edgelist\"\n",
    "    overlaps.append(-1.0)\n",
    "    for i in range(len(overlaps),89):\n",
    "        overlaps.append(-1.0)\n",
    "    projname = str(df_proj_social.loc[df_proj_social[\"project_aliase\"] == projid][\"project_name\"].values[0])\n",
    "    #print(projname)\n",
    "    df_social_overlaps[projname] = pd.Series(overlaps) \n",
    "df_social_overlaps.to_csv(\"./social_overlaps\"+str(time_resolution)+\".csv\", index=None, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "projects = os.listdir(e_path)\n",
    "project_names = [x.split('__')[0] for x in projects]\n",
    "project_names = pd.Series(project_names).drop_duplicates().values\n",
    "\n",
    "df_social_overlaps = pd.DataFrame()\n",
    "the_path = e_path\n",
    "for projid in project_names:\n",
    "    overlaps = []\n",
    "    prev_fname = the_path+projid+'__'+str(0)+\".edgelist\"\n",
    "    for seq_num in range(time_resolution,87, time_resolution):\n",
    "        #seq_num = 1\n",
    "        this_fname = the_path+projid+'__'+str(seq_num)+\".edgelist\"\n",
    "        \n",
    "        #while os.path.exists(this_fname):\n",
    "        if os.path.exists(prev_fname):\n",
    "            overlaps.append(get_net_jaccard(prev_fname, this_fname))\n",
    "        #seq_num+=1\n",
    "        prev_fname = this_fname\n",
    "        this_fname = the_path+projid+'__'+str(seq_num)+\".edgelist\"\n",
    "    overlaps.append(-1.0)\n",
    "    for i in range(len(overlaps),89):\n",
    "        overlaps.append(-1.0)\n",
    "    projname = str(df_proj_social.loc[df_proj_social[\"project_aliase\"] == projid][\"project_name\"].values[0])\n",
    "    #print(projname)\n",
    "    df_social_overlaps[projname] = pd.Series(overlaps) \n",
    "df_social_overlaps.to_csv(\"./social_jaccards\"+str(time_resolution)+\".csv\", index=None, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "projects = os.listdir('./network_data'+str(time_resolution)+'/mix/')\n",
    "project_names = [x.split('__')[0] for x in projects]\n",
    "project_names = pd.Series(project_names).drop_duplicates().values\n",
    "\n",
    "df_tech_overlaps = pd.DataFrame()\n",
    "the_path = './network_data'+str(time_resolution)+'/mix/'\n",
    "for projid in project_names:\n",
    "    overlaps = []\n",
    "    prev_fname = the_path+projid+'__'+str(0)+\".edgelist\"\n",
    "    for seq_num in range(time_resolution,63, time_resolution):\n",
    "        #seq_num = 1\n",
    "        this_fname = the_path+projid+'__'+str(seq_num)+\".edgelist\"\n",
    "        \n",
    "        #while os.path.exists(this_fname):\n",
    "        if os.path.exists(prev_fname):\n",
    "            overlaps.append(get_net_overlap(prev_fname, this_fname))\n",
    "        #seq_num+=1\n",
    "        prev_fname = this_fname\n",
    "        this_fname = the_path+projid+'__'+str(seq_num)+\".edgelist\"\n",
    "        \n",
    "    overlaps.append(-1.0)\n",
    "    for i in range(len(overlaps),65):\n",
    "        overlaps.append(-1.0)\n",
    "    projname = str(df_proj_tech.loc[df_proj_tech[\"project_aliase\"] == projid][\"project_name\"].values[0])\n",
    "    #print(projname)\n",
    "    df_tech_overlaps[projname] = pd.Series(overlaps) \n",
    "df_tech_overlaps.to_csv(\"./mix_overlaps\"+str(time_resolution)+\".csv\", index=None, sep=',')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "projects = os.listdir('./network_data'+str(time_resolution)+'/mix/')\n",
    "project_names = [x.split('__')[0] for x in projects]\n",
    "project_names = pd.Series(project_names).drop_duplicates().values\n",
    "\n",
    "df_tech_overlaps = pd.DataFrame()\n",
    "the_path = './network_data'+str(time_resolution)+'/mix/'\n",
    "for projid in project_names:\n",
    "    overlaps = []\n",
    "    prev_fname = the_path+projid+'__'+str(0)+\".edgelist\"\n",
    "    for seq_num in range(time_resolution,63, time_resolution):\n",
    "        #seq_num = 1\n",
    "        this_fname = the_path+projid+'__'+str(seq_num)+\".edgelist\"\n",
    "        \n",
    "        #while os.path.exists(this_fname):\n",
    "        if os.path.exists(prev_fname):\n",
    "            overlaps.append(get_net_jaccard(prev_fname, this_fname))\n",
    "        #seq_num+=1\n",
    "        prev_fname = this_fname\n",
    "        this_fname = the_path+projid+'__'+str(seq_num)+\".edgelist\"\n",
    "        \n",
    "    overlaps.append(-1.0)\n",
    "    for i in range(len(overlaps),65):\n",
    "        overlaps.append(-1.0)\n",
    "    projname = str(df_proj_tech.loc[df_proj_tech[\"project_aliase\"] == projid][\"project_name\"].values[0])\n",
    "    #print(projname)\n",
    "    df_tech_overlaps[projname] = pd.Series(overlaps) \n",
    "df_tech_overlaps.to_csv(\"./mix_jaccards\"+str(time_resolution)+\".csv\", index=None, sep=',')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'warble'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_name</th>\n",
       "      <th>project_aliase</th>\n",
       "      <th>project_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2421</th>\n",
       "      <td>Warble</td>\n",
       "      <td>warble</td>\n",
       "      <td>retired</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     project_name project_aliase project_status\n",
       "2421       Warble         warble        retired"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_proj_tech.loc[df_proj_tech[\"project_aliase\"] == projid]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c125d54415bf97f751e5832101874a2f084e84c850663dda323e1a70b27f8b7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('3.8.10': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

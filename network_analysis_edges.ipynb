{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import modin.pandas as mipd\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# technical nets are unweighted\n",
    "def get_tech_net(path):\n",
    "\n",
    "    bipartite_G = nx.Graph()\n",
    "    df = pd.read_csv(path, header=None, sep='##', engine='python')\n",
    "    df.columns = ['file', 'dev', 'weight']\n",
    "\n",
    "    ## Logic to add nodes and edges to graph with their metadata\n",
    "    for _, row in df.iterrows():\n",
    "        dev_node = row['dev']\n",
    "        file_node = row['file'].replace('   (with props)', '')\n",
    "        bipartite_G.add_node(dev_node, bipartite='dev')\n",
    "        bipartite_G.add_node(file_node, bipartite='file')\n",
    "        bipartite_G.add_edge(dev_node, file_node)\n",
    "\n",
    "    dev_nodes = {n for n, d in bipartite_G.nodes(data=True) if d[\"bipartite\"] == 'dev'}\n",
    "    file_nodes = {n for n, d in bipartite_G.nodes(data=True) if d[\"bipartite\"] == 'file'}\n",
    "    \n",
    "    return bipartite_G\n",
    "def cal_tech_net(path):\n",
    "    # check if file does not exist or empty\n",
    "    if not os.path.exists(path) or os.stat(path).st_size == 0:\n",
    "        return {'t_num_dev_nodes':0,\\\n",
    "                't_num_file_nodes':0,\\\n",
    "                't_num_dev_per_file':0,\\\n",
    "                't_num_file_per_dev':0,\\\n",
    "                't_graph_density':0,\\\n",
    "                't_dev_nodes': set()}\n",
    "\n",
    "    bipartite_G = get_tech_net(path)\n",
    "\n",
    "    graph_density = bipartite.density(bipartite_G, dev_nodes)\n",
    "    file_degrees, dev_degrees = bipartite.degrees(bipartite_G, dev_nodes)\n",
    "\n",
    "    num_file_nodes = len(file_degrees)\n",
    "    num_dev_nodes = len(dev_degrees)\n",
    "    file_node_degree = sum([degree for node, degree in file_degrees])/len(file_degrees)\n",
    "    dev_node_degree = sum([degree for node, degree in dev_degrees])/len(dev_degrees)\n",
    "\n",
    "    # return the features of tech net\n",
    "    return {'t_num_dev_nodes':num_dev_nodes,\\\n",
    "            't_num_file_nodes':num_file_nodes,\\\n",
    "            't_num_dev_per_file':file_node_degree,\\\n",
    "            't_num_file_per_dev':dev_node_degree,\\\n",
    "            't_graph_density':graph_density,\\\n",
    "            't_dev_nodes': set(dev_nodes)}\n",
    "\n",
    "def get_social_net(path):\n",
    "    G = nx.read_edgelist(path, create_using=nx.DiGraph(), nodetype=str, comments='*', delimiter='##', data=(('weight', int),))\n",
    "    return G\n",
    "\n",
    "# social nets are weighted\n",
    "def cal_social_net(path):\n",
    "    # if no network data\n",
    "    if not os.path.exists(path) or os.stat(path).st_size == 0:\n",
    "        return {'s_num_nodes':0, \\\n",
    "                's_dev_nodes':set(),\\\n",
    "                's_weighted_mean_degree':0,\\\n",
    "                's_num_component':0,\\\n",
    "                's_avg_clustering_coef':0,\\\n",
    "                's_largest_component':0,\\\n",
    "                's_graph_density':0}\n",
    "\n",
    "    # Processing features in social networks\n",
    "    G = nx.read_edgelist(path, create_using=nx.DiGraph(), nodetype=str, comments='*', delimiter='##', data=(('weight', int),))\n",
    "    # all dev nodes\n",
    "    dev_nodes = set(G.nodes)\n",
    "    # num. of total nodes\n",
    "    num_nodes = len(dev_nodes)\n",
    "    # weighted mean degree\n",
    "    degrees = G.degree(weight='weight')\n",
    "    weighted_mean_degree = sum([degree for node, degree in degrees])/num_nodes\n",
    "    # average clustering coefficient\n",
    "    avg_clustering_coef = nx.average_clustering(G)\n",
    "    # betweenness = nx.betweenness_centrality(G, weight='weight')\n",
    "    graph_density = nx.density(G)\n",
    "\n",
    "    G = nx.read_edgelist(path, create_using=nx.Graph(), nodetype=str, comments='*', delimiter='##', data=(('weight', int),))\n",
    "    # num. of dis-connected components\n",
    "    num_component = nx.number_connected_components(G)\n",
    "    # largest connected component\n",
    "    largest_component = len(max(nx.connected_components(G), key=len))\n",
    "    # num. of nodes in each component\n",
    "    # num_nodes_component = [list(c) for c in list(nx.connected_components(G))]\n",
    "\n",
    "    # return the features of the \n",
    "    return {'s_num_nodes': num_nodes,\\\n",
    "            's_dev_nodes': dev_nodes,\\\n",
    "            's_weighted_mean_degree':weighted_mean_degree,\\\n",
    "            's_num_component':num_component,\\\n",
    "            's_avg_clustering_coef':avg_clustering_coef,\\\n",
    "            's_largest_component':largest_component,\\\n",
    "            's_graph_density':graph_density}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_unite(net_tech, net_social, net_mix):\n",
    "\t# Merge tech network and social network by edgelist files\n",
    "\tif not os.path.exists(net_tech):\n",
    "\t\treturn None\n",
    "\tif not os.path.exists(net_social):\n",
    "\t\treturn None\n",
    "\n",
    "\tnet1_set = []\n",
    "\twith open(net_tech, 'r') as f:\n",
    "\t\tlines = f.read().splitlines()\n",
    "\t# print([net1, net2])\n",
    "\tfor line in lines:\n",
    "\t\tsender, recivier, weight = line.split('##')\n",
    "\t\tnet1_set.append([recivier, sender, weight])\n",
    "\tnet2_set = []\n",
    "\twith open(net_social, 'r') as f:\n",
    "\t\tlines = f.read().splitlines()\n",
    "\tfor line in lines:\n",
    "\t\tsender, recivier, weight = line.split('##')\t\t\n",
    "\t\tnet1_set.append([sender, recivier, weight])\n",
    "\n",
    "\t#print(net_mix_set)\n",
    "\twith open(net_mix, 'w') as f:\n",
    "\t\tfor sender, recivier, weight in net1_set:\n",
    "\t\t\tf.write(sender+\"##\"+recivier+\"##\"+weight+\"\\n\")\n",
    "\n",
    "\n",
    "\treturn net1_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_stats (netseries):\n",
    "    net_set = {}\n",
    "    for netname in netseries:\n",
    "        if not os.path.exists(netname):\n",
    "            #print(\"Not Exist File:\"+netname)\n",
    "            #netseries.append(set())\n",
    "            continue\n",
    "            # Don't Return NONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            #return None\n",
    "        else:\n",
    "            net_set = set()\n",
    "            with open(netname, 'r') as f:\n",
    "                lines = f.read().splitlines()\n",
    "            for line in lines:\n",
    "                sender, receiver, weight = line.split('##')\n",
    "                if sender+\"##\"+receiver in net_set:\n",
    "                    net_set[sender+\"##\"+receiver]+= 1\n",
    "                else:\n",
    "                    net_set[sender+\"##\"+receiver]= 1\n",
    "            netseries.append(net_set)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_graduated = np.load('all_graduated.npy').tolist()\n",
    "all_retired = np.load('all_retired.npy').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_resolution = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './network_data'+str(time_resolution)+'/'\n",
    "\n",
    "### Faster\n",
    "\n",
    "c_path = './network_data'+str(time_resolution)+'/commits/'\n",
    "e_path = './network_data'+str(time_resolution)+'/emails/'\n",
    "#c_path = './network_data/commits/'\n",
    "\n",
    "projects = os.listdir(c_path)\n",
    "project_names = [x.split('__')[0] for x in projects]\n",
    "project_names = pd.Series(project_names).drop_duplicates().values\n",
    "\n",
    "mix_path = data_path+\"mix/\"\n",
    "if not os.path.exists(mix_path):\n",
    "    os.makedirs(mix_path)\n",
    "\n",
    "for g_file in os.listdir(c_path):\n",
    "    net_mix_set = g_unite(c_path+g_file, e_path+g_file, mix_path+g_file)\n",
    "    #print(net_mix_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = os.listdir(c_path)\n",
    "project_names = [x.split('__')[0] for x in projects]\n",
    "project_names = pd.Series(project_names).drop_duplicates().values\n",
    "\n",
    "edge_history_set = {}\n",
    "the_path = c_path\n",
    "for projid in tqdm(project_names):\n",
    "    netlist = []\n",
    "    for seq_num in range(time_resolution, 43, time_resolution):\n",
    "        #seq_num = 1\n",
    "        this_fname = the_path+projid+'__'+str(seq_num)+\".edgelist\"\n",
    "        netlist.append(this_fname)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8c5435d0e76923f90691df99ce64c3b05b292deff7dde5a7010d3193cf829434"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('3.8.11': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
